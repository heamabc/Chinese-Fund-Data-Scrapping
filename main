# -*- coding: utf-8 -*-
# By Echo Kwan. Phone no. : 61255752, email :takhei611@gmail.com

# web scrapping
import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning
from bs4 import BeautifulSoup
from selenium import webdriver

# data transformation
import pandas as pd
import numpy as np

# misc
import warnings
from io import StringIO
import os
import time
from datetime import date

# ending email
import smtplib 
from email.mime.multipart import MIMEMultipart 
from email.mime.text import MIMEText 
from email.mime.base import MIMEBase 
from email import encoders 

# ignore warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

# To run the program, run this line in the bash: ./job.sh

# main function
def handler(event, context):
    
    # Funds part
    print('fund name...')
    fund_name = fund_name_function()
    print('NAV...')
    NAV = NAV_function()
    print('Company...')
    company = company_function()
    
    # merging the data together
    fund_output = NAV.merge(fund_name,how='outer', on='code')
    fund_output = fund_output.merge(company, how='left', on='code') # use left because the funds in fund_output are all the public funds, but right has other funds
    fund_output['fund'] = np.where(fund_output['name_x'].notnull(), fund_output['name_x'], np.where(fund_output['name_y'].notnull(), fund_output['name_y'], fund_output['fund']))
    fund_output['link'] = np.where(fund_output['link_x'].notnull(), fund_output['link_x'], fund_output['link_y'])
    
    fund_output = fund_output.rename(columns={'code':'Code', 'inception_ate':'Inception Date', 'manager':'Manager', 'number_of_unit':'Number Of Unit', 'per_unit_NAV':'Per Unit NAV', 'recent_unit':'Recent Unit', 'company':'Company','fund':'Fund','link':'Link'})
    fund_output = fund_output[['Code', 'Fund', 'Inception Date', 'Per Unit NAV', 'Number Of Unit', 'AUM', 'Manager', 'Company', 'Link']]
    fund_output[['Per Unit NAV', 'Number Of Unit', 'AUM']] = fund_output[['Per Unit NAV', 'Number Of Unit', 'AUM']].fillna(0,axis=1)
    fund_output.sort_values(by=['Code'], inplace=True)
    
    print('Fund Done')
    
    # News part
    print('industry news...')
    industry_news = industry_news_function()
    print('fund announcement...')
    fund_announcement = fund_announcement_function(date.today().strftime('%Y-%m-%d'))
    print('SAFE...')
    SAFE = SAFE_function()
    print('PBOC...')
    PBOC = PBOC_function()
    
    news_output = pd.concat([industry_news,fund_announcement], ignore_index=True)
    news_output = pd.concat([news_output, SAFE], ignore_index=True)
    news_output = pd.concat([news_output, PBOC], ignore_index = True)
    
    news_output = news_output[['Date', 'News', 'News Type', 'Related Fund', 'Link']]
    
    print('News Done')
    
    # Send Email
    file_list = []
    file_name_list = []
    
    s = StringIO()
    fund_output.to_csv(s, encoding='utf-8', index=False)
    file_list.append(s)
    file_name_list.append('funds_data.csv')
    
    s = StringIO()
    news_output.to_csv(s, encoding='utf-8', index=False)
    file_list.append(s)
    file_name_list.append('news_data.csv')
    
    send_email(file_list, file_name_list)
    
    return 
    
def fund_name_function():
    # To take the name of all public funds in China to ensure that we do not miss any funds
    # return code, name and link

    url = 'http://money.finance.sina.com.cn/fund/view/vNewFund_FundListings.php'
    page = requests.get(url)
    
    soup = BeautifulSoup(page.content,'html.parser', fromEncoding="gb18030")
    soup = soup.find_all("div", id="s2")[0]
    soup = soup.find_all('a', title=True)
    # The last one is useless
    soup = soup[:-1]
    
    code = [a['title'][-7:-1] for a in soup]
    fund_name = [a['title'][:-8] for a in soup]
    link = [a['href'] for a in soup]
    
    output = pd.DataFrame([code, fund_name,link]).T
    output.columns = ['code','name','link']
    
    return output
    
def industry_news_function():
    # To take industry news of public funds in China
    # return date, news, link, news type
    url = 'http://finance.sina.com.cn/roll/index.d.html?cid=56948&page=1'
    page = requests.get(url)
    
    soup = BeautifulSoup(page.content, 'html.parser', fromEncoding="gb18030")
    soup = soup.find_all('li')
    soup = [ele for ele in soup if ele.span != None]
    
    industry_news = []
    dates = []
    link = []
    for ele in soup:
        industry_news.append(ele.contents[0].contents[0])
        dates.append('2019-' + ele.span.contents[0][1:6].replace('月','-'))
        link.append(soup[0].contents[0]['href'])
        
    output = pd.DataFrame({'news':industry_news, 'link':link, 'date':dates})
    output['news_type'] = 'Industry News'
    return output
    
def fund_announcement_function(date):
    # To take all the fund announcement by date
    # date = 'YYYY-MM-DD'
    # return date, fund_announcement, related fund, link, news_type
    url = 'http://biz.finance.sina.com.cn/fundinfo/share/jjgg_search.php?content={}&type=same_date'.format(date)
    page = requests.get(url)
    
    soup = BeautifulSoup(page.content, 'html.parser', fromEncoding="gb18030")
    big_soup = soup.find_all("table", class_="datalist")[0]
    
    soup = big_soup.find_all('a')
    
    fund_name = []
    heading = []
    link = []
    for i in range(len(soup)):
        if i%4 == 0:
            fund_name.append(soup[i].contents[0])
        elif (i-2)%4 == 0:
            heading.append(soup[i].contents[0])
            link.append(soup[i]['href'])
    
    soup = big_soup.find_all('th', style="border-right:1px #ccc solid;")
    type_list = []
    for a in soup:
        if isinstance(a.contents[0], str):
            type_list.append(a.contents[0])
    
    output = pd.DataFrame({'Related Fund':fund_name, 'News':heading, 'Link':link})
    output['Date'] = date
    output['News Type'] = 'Fund Announcement - ' + pd.Series(type_list)
    return output
    
def NAV_function():
    # To take the name, code, AUM (in millions), NAV, number of units, manager, inception date and link of the funds
    url = 'http://vip.stock.finance.sina.com.cn/fund_center/index.html#jjgmall'
    driver = webdriver.PhantomJS(r"/home/ubuntu/environment/~/.local/share/virtualenvs/environment-_B8sq8XY/lib/python3.6/site-packages/phantomjs_2.1.1", service_log_path=os.path.devnull)
    driver.get(url)
    
    output = pd.DataFrame()
    while True:        
        #make the source text to list of list
        table = driver.find_element_by_tag_name('tbody')
        table = table.text.split("\n")
        table = [ele.split(" ") for ele in table]
    
        #transform list of list of data of the funds to table
        tmp_output = pd.DataFrame(table)
        tmp_output = tmp_output.iloc[:,1:8]
        tmp_output.columns=(['code', 'name', 'per_unit_NAV', 'number_of_unit', 'AUM', 'inception_ate', 'manager'])
        
        output = pd.concat([output, tmp_output], ignore_index=True)
        
        #clicking next page and sleep
        try:
            next_page = driver.find_element_by_link_text('下一页')
            next_page.location_once_scrolled_into_view
            next_page.click()
        except:
            #If there is not 'next page' in the web to click, we will exit the loop.
            break
        
        time.sleep(1)

    output['number_of_unit'] = np.where(output['number_of_unit'] == '--', 0, output['number_of_unit'])
    output['per_unit_NAV'] = np.where(output['per_unit_NAV'] == '--', 0, output['per_unit_NAV'])
    output['AUM'] = np.where(output['AUM'] == '--', 0, output['AUM'].str.replace(',','').astype(int) * 100)
    return output
    
def company_function():
    # To match the company in which all the funds belong to
    # return code, company, fund name, link
    
    url = 'http://money.finance.sina.com.cn/fund/view/vNewFund_FundCompanyListings.php'
    page = requests.get(url)
    
    soup = BeautifulSoup(page.content,'html.parser', fromEncoding="gb18030")
    tables = soup.find_all("table", width="100%")
    companies = soup.find_all('div', class_='s11')
    
    # Take all the codes and name of funds to ensure that we do not miss any funds
    output = pd.DataFrame()
    for i in range(len(tables)):
        company = companies[i].text
        fund = tables[i].find_all('a', title=True)
        tmp_output = pd.DataFrame([[company] * len(fund), [ele.text for ele in fund], [ele['href'] for ele in fund]]).T
        output = pd.concat([output,tmp_output], ignore_index=True)
        
    output.columns=['company', 'fund', 'link']
    output['code'] = output['fund'].str[-7:-1]
    output['fund'] = output['fund'].str[:-8]
    return output
    
def PBOC_function():
    # To take the reports and publications published by PBOC
    # return news, link, date, news_type
    law = 'http://www.pbc.gov.cn/tiaofasi/144941/144951/index.html'
    administration = 'http://www.pbc.gov.cn/tiaofasi/144941/144953/index.html'
    rules = 'http://www.pbc.gov.cn/tiaofasi/144941/144957/index.html'
    documentation = 'http://www.pbc.gov.cn/tiaofasi/144941/3581332/index.html'
    others = 'http://www.pbc.gov.cn/tiaofasi/144941/144959/index.html'
    urls = (law, administration, rules, documentation, others)
    names = ('国家法律', '行政法规', '部门规章', '主要有效规范性文件', '其他文件')
    
    output = pd.DataFrame()
    for i in range(len(urls)):
        url = urls[i]
        name = names[i]
        
        driver = webdriver.PhantomJS(r'/home/ubuntu/environment/~/.local/share/virtualenvs/environment-_B8sq8XY/lib/python3.6/site-packages/phantomjs_2.1.1', service_log_path=os.path.devnull)
        driver.get(url)
        
        time.sleep(10)
        driver.refresh()
        
        table = driver.find_element_by_id('r_con')
        news = table.text.split('\n')
        
        news = news[:-2]
        news[0] = news[0].strip()
        date = [ele[-10:] for ele in news]
        
        soup = BeautifulSoup(driver.page_source, 'html.parser', fromEncoding="gb18030")
        link = soup.find_all('td', align='left', height='22')
        link = ['http://www.pbc.gov.cn' + ele.contents[0].contents[0]['href'] for ele in link]
        
        tmp_output = pd.DataFrame([news, date, link]).T
        tmp_output.columns = ['News', 'Date', 'Link']
        tmp_output['News Type'] = 'PBOC - ' + name
        output = pd.concat([output, tmp_output], ignore_index=True)
    
    output.dropna()
    return output
    
def SAFE_function():
    # To take the reports and publications published by SAFE
    # return news, link, date, news_type
    url = 'http://www.safe.gov.cn/safe/zcfg/index.html'
    page = requests.get(url)
    
    soup = BeautifulSoup(page.content, 'html.parser', fromEncoding="gb18030")
    soup = soup.find_all("div", class_="main_m")[0]
    soup = soup.find_all("div", class_="right_list")[0]
    dates = [ele.contents[0] for ele in soup.find_all('dd')]
    soup = soup.find_all('a', title=True, href=True)
    
    output = {}
    for a in soup:
        output[a['title']] = 'www.safe.gov.cn/' + a['href']
    
    output.pop('下一页', None)
    output.pop('尾页', None )
    
    output = pd.DataFrame(output.items(), columns=['News','Link'])
    output['Date'] = dates
    output['News Type'] = 'SAFE'
    return output
    
def IgnitesAsia():
    url = 'https://www.ignitesasia.com/'
    driver = webdriver.PhantomJS(r'/home/ubuntu/environment/~/.local/share/virtualenvs/environment-_B8sq8XY/lib/python3.6/site-packages/phantomjs_2.1.1', service_log_path=os.path.devnull)
    driver.get(url)

    login = driver.find_elements_by_class_name('btn')
    login = login[3]
    login.click()

    time.sleep(5)

    username = driver.find_element_by_id('username')
    password = driver.find_element_by_id('password')
    login = driver.find_element_by_tag_name('button')

    username.send_keys('louis.tung@allianzgi.com')
    password.send_keys('Ignites123')
    login.click()

    time.sleep(5)

    driver.get('https://www.ignitesasia.com/category/170')

    news = driver.find_element_by_id('main')

    titles = news.find_elements_by_tag_name('h4')
    links = [ele.find_element_by_tag_name('a').get_attribute('href') for ele in titles]
    titles = [ele.text for ele in titles]


    dates = news.find_elements_by_tag_name('p')
    dates = [ele.text[:-11] for ele in dates]
    dates = [datetime.datetime.strptime(ele, '%B %d, %Y').strftime('%Y-%m-%d') for ele in dates]

    summary = []
    for x in range(1,11):
        summary.append(news.find_element_by_xpath('//*[@id="main"]/div/div[{}]/div/div'.format(x)).text)

    output = pd.DataFrame([dates, titles, summary, links]).T
    output.columns = ['Date', 'News', 'Summary', 'Link']
    output['News Type'] = 'IgniteAsia'
    
    return output
    
def send_email(file_list, file_name_list):
    fromaddr = "allianznewsproject@gmail.com"
    toaddr = "1155093246@link.cuhk.edu.hk"
    
    msg = MIMEMultipart() 
    msg['From'] = fromaddr 
    msg['To'] = toaddr 
    
    msg['Subject'] = "Funds and News data from Echo Kwan CUHK"
    body = ""
    msg.attach(MIMEText(body, 'plain')) 
    
    for i in range(len(file_list)):
        filename = file_name_list[i]
        attachment = file_list[i]
    
        p = MIMEBase('application', 'octet-stream') 
        p.set_payload(attachment.getvalue().encode('utf-8')) 
        encoders.encode_base64(p) 
        p.add_header('Content-Disposition', "attachment; filename= %s" % filename) 
        msg.attach(p) 
    
    s = smtplib.SMTP('smtp.gmail.com', 587) 
    s.starttls() 
    s.login(fromaddr, "heyheycantakea")
    
    text = msg.as_string() 
    s.sendmail(fromaddr, toaddr, text) 
    s.quit() 
    return



if __name__== "__main__":
    handler(0,0)
